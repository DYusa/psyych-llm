{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd0qr8FpSfcv",
        "outputId": "4a205425-577a-4b6e-e071-e91785f0d0f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import random\n",
        "random.seed(10)\n",
        "import re\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from random import *\n",
        "from IPython.display import Image, display\n",
        "\n",
        "file_path = '/content/drive/MyDrive/text.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    text_data = file.read()\n",
        "\n",
        "type(text_data)\n",
        "print(text_data)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "sentences = re.sub(\"[.,!?\\\\-]\", '', text_data.lower()).split('\\n')\n",
        "word_list = list(set(\" \".join(sentences).split()))\n",
        "print(word_list)\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "\n",
        "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
        "print(word_dict)\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "\n",
        "for i, w in enumerate(word_list):\n",
        "    word_dict[w] = i + 4\n",
        "print(word_dict)\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "number_dict = {i: w for i, w in enumerate(word_dict)}\n",
        "print(number_dict)\n",
        "\n",
        "vocab_size = len(word_dict)\n",
        "print(vocab_size)\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "token_list = list()\n",
        "\n",
        "for sentence in sentences:\n",
        "    arr = [word_dict[s] for s in sentence.split()]\n",
        "    token_list.append(arr)\n",
        "print(token_list)\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "text_data[0:42]\n",
        "token_list[0]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw9vmSaGOrl9",
        "outputId": "a3ac769b-14ab-48e2-dcd7-b6c177241c35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\"Hello, how are you? I am Carol.\\n\"\n",
            "\"Hello, Carol, my name is Frank. Nice to meet you.\\n\"\n",
            "\"Nice to meet you too. How are you today?\\n\"\n",
            "\"Great. My football team won the competition.\\n\"\n",
            "\"Wow, congratulations Frank!\\n\"\n",
            "\"Thank you Carol.\\n\"\n",
            "\"Shall we have a pizza later to celebrate?\\n\"\n",
            "\"Sure. Do you recommend any restaurant, Carol?\\n\"\n",
            "\"Yes, a new restaurant opened, and they say the banana pizza is phenomenal.\\n\"\n",
            "\"Okay. Let's meet at the restaurant at seven PM, is that okay?\\n\"\n",
            "\"That's fine. See you later then.\"\n",
            "\n",
            "\n",
            "['restaurant', 'congratulations', 'is', '\"wow', 'and', 'say', 'meet', 'how', '\"sure', 'am', 'you', 'then\"', 'won', 'to', 'frank', 'opened', 'seven', '\"yes', '\"nice', 'name', 'too', 'i', 'a', 'any', 'at', 'do', 'pizza', \"let's\", 'carol\\\\n\"', 'are', 'competition\\\\n\"', '\"thank', 'phenomenal\\\\n\"', '\"okay', 'the', 'banana', 'okay\\\\n\"', '\"that\\'s', 'new', 'that', 'my', 'fine', 'frank\\\\n\"', 'celebrate\\\\n\"', 'pm', 'team', '\"shall', 'later', 'you\\\\n\"', '\"hello', 'today\\\\n\"', 'have', 'recommend', 'they', 'see', '\"great', 'football', 'we', 'carol', 'nice']\n",
            "---------------------------------------\n",
            "{'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
            "---------------------------------------\n",
            "{'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3, 'restaurant': 4, 'congratulations': 5, 'is': 6, '\"wow': 7, 'and': 8, 'say': 9, 'meet': 10, 'how': 11, '\"sure': 12, 'am': 13, 'you': 14, 'then\"': 15, 'won': 16, 'to': 17, 'frank': 18, 'opened': 19, 'seven': 20, '\"yes': 21, '\"nice': 22, 'name': 23, 'too': 24, 'i': 25, 'a': 26, 'any': 27, 'at': 28, 'do': 29, 'pizza': 30, \"let's\": 31, 'carol\\\\n\"': 32, 'are': 33, 'competition\\\\n\"': 34, '\"thank': 35, 'phenomenal\\\\n\"': 36, '\"okay': 37, 'the': 38, 'banana': 39, 'okay\\\\n\"': 40, '\"that\\'s': 41, 'new': 42, 'that': 43, 'my': 44, 'fine': 45, 'frank\\\\n\"': 46, 'celebrate\\\\n\"': 47, 'pm': 48, 'team': 49, '\"shall': 50, 'later': 51, 'you\\\\n\"': 52, '\"hello': 53, 'today\\\\n\"': 54, 'have': 55, 'recommend': 56, 'they': 57, 'see': 58, '\"great': 59, 'football': 60, 'we': 61, 'carol': 62, 'nice': 63}\n",
            "---------------------------------------\n",
            "{0: '[PAD]', 1: '[CLS]', 2: '[SEP]', 3: '[MASK]', 4: 'restaurant', 5: 'congratulations', 6: 'is', 7: '\"wow', 8: 'and', 9: 'say', 10: 'meet', 11: 'how', 12: '\"sure', 13: 'am', 14: 'you', 15: 'then\"', 16: 'won', 17: 'to', 18: 'frank', 19: 'opened', 20: 'seven', 21: '\"yes', 22: '\"nice', 23: 'name', 24: 'too', 25: 'i', 26: 'a', 27: 'any', 28: 'at', 29: 'do', 30: 'pizza', 31: \"let's\", 32: 'carol\\\\n\"', 33: 'are', 34: 'competition\\\\n\"', 35: '\"thank', 36: 'phenomenal\\\\n\"', 37: '\"okay', 38: 'the', 39: 'banana', 40: 'okay\\\\n\"', 41: '\"that\\'s', 42: 'new', 43: 'that', 44: 'my', 45: 'fine', 46: 'frank\\\\n\"', 47: 'celebrate\\\\n\"', 48: 'pm', 49: 'team', 50: '\"shall', 51: 'later', 52: 'you\\\\n\"', 53: '\"hello', 54: 'today\\\\n\"', 55: 'have', 56: 'recommend', 57: 'they', 58: 'see', 59: '\"great', 60: 'football', 61: 'we', 62: 'carol', 63: 'nice'}\n",
            "64\n",
            "---------------------------------------\n",
            "[[53, 11, 33, 14, 25, 13, 32], [53, 62, 44, 23, 6, 18, 63, 17, 10, 52], [22, 17, 10, 14, 24, 11, 33, 14, 54], [59, 44, 60, 49, 16, 38, 34], [7, 5, 46], [35, 14, 32], [50, 61, 55, 26, 30, 51, 17, 47], [12, 29, 14, 56, 27, 4, 32], [21, 26, 42, 4, 19, 8, 57, 9, 38, 39, 30, 6, 36], [37, 31, 10, 28, 38, 4, 28, 20, 48, 6, 43, 40], [41, 45, 58, 14, 51, 15]]\n",
            "---------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[53, 11, 33, 14, 25, 13, 32]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 24. Hyperparameters\n",
        "batch_size = 10\n",
        "n_segments = 2\n",
        "dropout = 0.2\n",
        "\n",
        "# Maximum length\n",
        "maxlen = 100\n",
        "\n",
        "# Maximum number of tokens to be predicted\n",
        "max_pred = 7\n",
        "\n",
        "# Number of layers\n",
        "n_layers = 8\n",
        "\n",
        "# Number of heads in multi-head attention\n",
        "n_heads = 12\n",
        "\n",
        "# Embedding size\n",
        "d_model = 768\n",
        "\n",
        "# Feedforward dimension size: 4 * d_model\n",
        "d_ff = d_model * 4\n",
        "\n",
        "# Dimension of K(=Q)V\n",
        "d_k = d_v = 64\n",
        "\n",
        "# Epochs\n",
        "NUM_EPOCHS = 50"
      ],
      "metadata": {
        "id": "g5C5idqvO6I2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25. Define the function to create data batches\n",
        "def make_batch():\n",
        "\n",
        "    # Initialize the batch as an empty list\n",
        "    batch = []\n",
        "\n",
        "    # Initialize counters for positive and negative examples\n",
        "    positive = negative = 0\n",
        "\n",
        "    # Continue until half of the batch is positive examples and the other half is negative examples\n",
        "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
        "\n",
        "        # Randomly select indices for two sentences\n",
        "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
        "\n",
        "        # Retrieve the tokens corresponding to the indices\n",
        "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "\n",
        "        # Prepare the input ids by adding special tokens [CLS] and [SEP]\n",
        "        input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
        "\n",
        "        # Define the segment ids to differentiate the two sentences\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        # Calculate the number of predictions to be made (15% of tokens)\n",
        "        n_pred = min(max_pred, max(1, int(round(len(input_ids) * 0.15))))\n",
        "\n",
        "        # Identify candidate positions for masking that are not [CLS] or [SEP]\n",
        "        cand_masked_pos = [i for i, token in enumerate(input_ids) if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
        "\n",
        "        # Shuffle the candidate positions\n",
        "        shuffle(cand_masked_pos)\n",
        "\n",
        "        # Initialize lists for masked tokens and their positions\n",
        "        masked_tokens, masked_pos = [], []\n",
        "\n",
        "        # Mask tokens until the desired number of predictions is reached\n",
        "        for pos in cand_masked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "\n",
        "            # Random mask\n",
        "            if random() < 0.8:\n",
        "                input_ids[pos] = word_dict['[MASK]']\n",
        "\n",
        "            # Replace with another token 10% of the time (20% of the remaining time)\n",
        "            elif random() < 0.5:\n",
        "                index = randint(0, vocab_size - 1)\n",
        "                input_ids[pos] = word_dict[number_dict[index]]\n",
        "\n",
        "        # Add zero padding to the input ids and segment ids to reach the maximum length\n",
        "        n_pad = maxlen - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "        # Add zero padding to the masked tokens and their positions if needed\n",
        "        if max_pred > n_pred:\n",
        "            n_pad = max_pred - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "\n",
        "        # Add to the batch as a positive example if the sentences are consecutive\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
        "            positive += 1\n",
        "\n",
        "        # Add to the batch as a negative example if the sentences are not consecutive\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
        "            negative += 1\n",
        "\n",
        "    # Return the complete batch\n",
        "    return batch"
      ],
      "metadata": {
        "id": "4SeQExg7O6Ph"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26. Function for padding\n",
        "def get_attn_pad_masked(seq_q, seq_k):\n",
        "\n",
        "    batch_size, len_q = seq_q.size()\n",
        "\n",
        "    batch_size, len_k = seq_k.size()\n",
        "\n",
        "    pad_attn_masked = seq_k.data.eq(0).unsqueeze(1)\n",
        "\n",
        "    return pad_attn_masked.expand(batch_size, len_q, len_k)\n",
        "# 27. Create a batch\n",
        "batch = make_batch()\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "input_ids\n",
        "print(\"?????????????\")\n",
        "input_ids[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL-xXuuOO6Zw",
        "outputId": "e6f3621c-a859-4d34-ce3c-5d023a661252"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "?????????????\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 1, 37, 31, 10, 28, 38,  3, 28, 20, 48,  6, 43,  3,  2, 53, 11, 33, 14,\n",
              "        25, 13, 32,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_tokens[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrDwLMEaO6co",
        "outputId": "50565e93-0bb3-47eb-8cc4-58834c721b2d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([28, 40,  4,  0,  0,  0,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_pos[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ByB6xxWO6e6",
        "outputId": "24775801-a281-4608-c051-942a06f27b5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 4, 12,  6,  0,  0,  0,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "isNext[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hbxfrycRMUB",
        "outputId": "06f17271-7261-4090-a3f8-249207345a4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_attn_pad_masked(input_ids, input_ids)[0][0], input_ids[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9x-PS1gRMJ-",
        "outputId": "79a6de00-eb30-47c0-fb77-fa0c95e6a291"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True]),\n",
              " tensor([ 1, 37, 31, 10, 28, 38,  3, 28, 20, 48,  6, 43,  3,  2, 53, 11, 33, 14,\n",
              "         25, 13, 32,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 35. GeLU activation function\n",
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ],
      "metadata": {
        "id": "-rjQBa6lRMMl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 36. Embedding Class\n",
        "class Embedding(nn.Module):\n",
        "\n",
        "    # Constructor method\n",
        "    def __init__(self):\n",
        "\n",
        "        super(Embedding, self).__init__()\n",
        "\n",
        "        # Token embedding\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Position embedding\n",
        "        self.pos_embed = nn.Embedding(maxlen, d_model)\n",
        "\n",
        "        # Segment (token type) embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    # Forward method\n",
        "    def forward(self, x, seg):\n",
        "\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        pos = torch.arange(seq_len, dtype=torch.long)\n",
        "\n",
        "        # (seq_len,) -> (batch_size, seq_len)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)\n",
        "\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "\n",
        "        return self.norm(embedding)"
      ],
      "metadata": {
        "id": "E4o7NmWBRMPJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 37. Define the class to perform Scaled Dot-Product Attention\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "\n",
        "    # Initialization method for the class\n",
        "    def __init__(self):\n",
        "\n",
        "        # Initialize the base class\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    # Forward method to define the data pass through\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "\n",
        "        # Calculate the attention scores as the dot product of Q and K, and normalize by the key size\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\n",
        "\n",
        "        # Apply the attention mask to prevent attention to certain tokens\n",
        "        scores.masked_fill_(attn_mask, -1e9)\n",
        "\n",
        "        # Apply softmax to get normalized attention weights\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "\n",
        "        # Multiply the attention weights by V to get the context\n",
        "        context = torch.matmul(attn, V)\n",
        "\n",
        "        # Return the context and the attention weights\n",
        "        return context, attn"
      ],
      "metadata": {
        "id": "FyqnuTLGRMRg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 38. Define the class to perform multi-head attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        # Initialize the base class\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        # Define the weight matrix for the queries Q\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "\n",
        "        # Define the weight matrix for the keys K\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "\n",
        "        # Define the weight matrix for the values V\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "\n",
        "    # Forward method to define the data pass through\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "\n",
        "        # Save the input Q for the residual and get the batch size\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "\n",
        "        # Process Q through W_Q and reshape the result to have [n_heads] in the second dimension\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
        "\n",
        "        # Process K through W_K and reshape the result to have [n_heads] in the second dimension\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\n",
        "\n",
        "        # Process V through W_V and reshape the result to have [n_heads] in the second dimension\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1, 2)\n",
        "\n",
        "        # Adapt attn_mask to be compatible with the dimensions of q_s, k_s, v_s\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
        "\n",
        "        # Calculate the scaled dot-product attention and context for each attention head\n",
        "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "\n",
        "        # Reshape the context to combine the attention heads and return to the original format\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\n",
        "\n",
        "        # Apply a linear transformation to the combined context\n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "\n",
        "        # Normalize the output layer and add the residual\n",
        "        return nn.LayerNorm(d_model)(output + residual), attn"
      ],
      "metadata": {
        "id": "80jFrRLlRf73"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb = Embedding()\n",
        "embeds = emb(input_ids, segment_ids)\n",
        "attenM = get_attn_pad_masked(input_ids, input_ids)\n",
        "MHA = MultiHeadAttention()(embeds, embeds, embeds, attenM)\n",
        "output, A = MHA\n",
        "A[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzytAOASRf-w",
        "outputId": "0ef54699-877e-43ea-8ab5-32040101e52f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0525, 0.0465, 0.0283,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0427, 0.0414, 0.0452,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0507, 0.0469, 0.0286,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        ...,\n",
              "        [0.0412, 0.0487, 0.0704,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0496, 0.0552, 0.0680,  ..., 0.0000, 0.0000, 0.0000],\n",
              "        [0.0397, 0.0436, 0.0572,  ..., 0.0000, 0.0000, 0.0000]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 45. Define the class for the Positional Feed Forward Network\n",
        "class PoswiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        # Initialize the base class\n",
        "        super(PoswiseFeedForward, self).__init__()\n",
        "\n",
        "        # First linear layer that increases the data dimension from d_model to d_ff\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "\n",
        "        # Second linear layer that reduces the dimension back from d_ff to d_model\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    # Forward method to define the data pass through\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Apply the first linear transformation, followed by the GELU activation function,\n",
        "        # and then the second linear transformation\n",
        "        return self.fc2(gelu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "fnCGu-GfRgBa"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 46. Define the class for the encoder layer\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "\n",
        "        # Initialize the base class\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        # Instantiate multi-head attention for the encoder self-attention\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "\n",
        "        # Instantiate the Positional Feed Forward network to use after self-attention\n",
        "        self.pos_ffn = PoswiseFeedForward()\n",
        "\n",
        "    # Forward method to define the data pass through\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "\n",
        "        # Apply self-attention to the input data\n",
        "        enc_inputs, atnn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
        "\n",
        "        # After self-attention, pass the result through the Positional Feed Forward network\n",
        "        enc_inputs = self.pos_ffn(enc_inputs)\n",
        "\n",
        "        # Return the encoder output and attention weights\n",
        "        return enc_inputs, atnn"
      ],
      "metadata": {
        "id": "_-0WbAmFRgDz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 47. BERT Model\n",
        "class BERT(nn.Module):  # Define the BERT model\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        # Initialize the base class nn.Module\n",
        "        super(BERT, self).__init__()\n",
        "\n",
        "        # Embedding module for tokens, positions, and segments\n",
        "        self.embedding = Embedding()\n",
        "\n",
        "        # Encoder layers using the EncoderLayer class\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "\n",
        "        # Fully connected layer for linear transformation\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Activation function Tanh\n",
        "        self.activ1 = nn.Tanh()\n",
        "\n",
        "        # Additional linear layer\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Activation function GeLU\n",
        "        self.activ2 = gelu\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "\n",
        "        # Initialize the decoder and share weights with token embeddings\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "        self.decoder.weight = embed_weight\n",
        "\n",
        "        # Initialize decoder bias\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        # Generate embeddings for input tokens and segments\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "\n",
        "        # Generate attention mask to handle padding tokens\n",
        "        enc_self_attn_mask = get_attn_pad_masked(input_ids, input_ids)\n",
        "\n",
        "        # Pass embeddings through encoder layers\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "\n",
        "        # Apply pooling to the [CLS] token representation\n",
        "        h_pooled = self.activ1(self.fc(output[:, 0]))\n",
        "\n",
        "        # Generate classification logits\n",
        "        logits_clsf = self.classifier(h_pooled)\n",
        "\n",
        "        # Expand masked positions for attention\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1))\n",
        "\n",
        "        # Gather masked token representations\n",
        "        h_masked = torch.gather(output, 1, masked_pos)\n",
        "\n",
        "        # Apply activation and normalization to the masked tokens\n",
        "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
        "\n",
        "        # Decode masked token representations to logits\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
        "\n",
        "        # Return logits for masked language modeling and classification\n",
        "        return logits_lm, logits_clsf\n"
      ],
      "metadata": {
        "id": "KYugmxUgRvrq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BERT()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "batch = make_batch()\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))"
      ],
      "metadata": {
        "id": "b5Oa2oQxRyua"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Start the training loop for a defined number of epochs\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    # Zero the gradients of the optimizer to avoid accumulation from previous epochs\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Pass the input data through the model and get the logits for language masking\n",
        "    # and next sentence classification\n",
        "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "\n",
        "    # Calculate the loss for the language modeling task by comparing the predicted logits\n",
        "    # with the true tokens\n",
        "    loss_lm = criterion(logits_lm.transpose(1,2), masked_tokens)\n",
        "\n",
        "    # Compute the mean of the loss for normalization\n",
        "    loss_lm = (loss_lm.float()).mean()\n",
        "\n",
        "    # Calculate the loss for the next sentence classification task\n",
        "    loss_clsf = criterion(logits_clsf, isNext)\n",
        "\n",
        "    # Sum the losses from both tasks to get the total loss\n",
        "    loss = loss_lm + loss_clsf\n",
        "\n",
        "    # Display the current epoch and total loss\n",
        "    print(f'Epoch: {epoch + 1} | Loss {loss:.4f}')\n",
        "\n",
        "    # Perform backpropagation to compute the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the model parameters based on the calculated gradients\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McuTfRe9R36o",
        "outputId": "1628dcbc-0315-4cfc-d072-1b576b12f430"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Loss 84.7126\n",
            "Epoch: 2 | Loss 122.1345\n",
            "Epoch: 3 | Loss 284.4071\n",
            "Epoch: 4 | Loss 73.3592\n",
            "Epoch: 5 | Loss 195.4119\n",
            "Epoch: 6 | Loss 173.6462\n",
            "Epoch: 7 | Loss 148.2958\n",
            "Epoch: 8 | Loss 115.4070\n",
            "Epoch: 9 | Loss 87.5349\n",
            "Epoch: 10 | Loss 67.6660\n",
            "Epoch: 11 | Loss 46.9813\n",
            "Epoch: 12 | Loss 29.2772\n",
            "Epoch: 13 | Loss 25.2530\n",
            "Epoch: 14 | Loss 29.2006\n",
            "Epoch: 15 | Loss 30.7385\n",
            "Epoch: 16 | Loss 30.0904\n",
            "Epoch: 17 | Loss 29.1983\n",
            "Epoch: 18 | Loss 26.2729\n",
            "Epoch: 19 | Loss 23.9714\n",
            "Epoch: 20 | Loss 20.8577\n",
            "Epoch: 21 | Loss 20.2223\n",
            "Epoch: 22 | Loss 18.7058\n",
            "Epoch: 23 | Loss 18.3251\n",
            "Epoch: 24 | Loss 17.2492\n",
            "Epoch: 25 | Loss 17.1634\n",
            "Epoch: 26 | Loss 15.8550\n",
            "Epoch: 27 | Loss 15.4178\n",
            "Epoch: 28 | Loss 14.7199\n",
            "Epoch: 29 | Loss 13.8990\n",
            "Epoch: 30 | Loss 15.1159\n",
            "Epoch: 31 | Loss 13.2404\n",
            "Epoch: 32 | Loss 11.9363\n",
            "Epoch: 33 | Loss 11.9737\n",
            "Epoch: 34 | Loss 11.6567\n",
            "Epoch: 35 | Loss 11.0806\n",
            "Epoch: 36 | Loss 10.0341\n",
            "Epoch: 37 | Loss 9.6127\n",
            "Epoch: 38 | Loss 8.8871\n",
            "Epoch: 39 | Loss 8.1047\n",
            "Epoch: 40 | Loss 7.5915\n",
            "Epoch: 41 | Loss 7.0486\n",
            "Epoch: 42 | Loss 6.8577\n",
            "Epoch: 43 | Loss 7.2486\n",
            "Epoch: 44 | Loss 7.0869\n",
            "Epoch: 45 | Loss 7.0844\n",
            "Epoch: 46 | Loss 7.0116\n",
            "Epoch: 47 | Loss 6.7644\n",
            "Epoch: 48 | Loss 7.8634\n",
            "Epoch: 49 | Loss 6.5630\n",
            "Epoch: 50 | Loss 6.6303\n",
            "CPU times: user 6min 15s, sys: 6.64 s, total: 6min 21s\n",
            "Wall time: 6min 32s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 53. Extract the batch\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
        "print(text_data)\n",
        "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
        "# 54. Extract predictions of the tokens\n",
        "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
        "print('List of Real Masked Tokens: ', [pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
        "print('List of Predicted Masked Tokens: ', [pos for pos in logits_lm if pos != 0])\n",
        "# 55. Extract predictions of the next token\n",
        "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
        "print('isNext (True Value): ', True if isNext else False)\n",
        "print('isNext (Predicted Value): ', True if logits_clsf else False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgXYM8U0R5oh",
        "outputId": "21a8c569-59b9-4d06-bfe5-db1eeec44c03"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Hello, how are you? I am Carol.\\n\"\n",
            "\"Hello, Carol, my name is Frank. Nice to meet you.\\n\"\n",
            "\"Nice to meet you too. How are you today?\\n\"\n",
            "\"Great. My football team won the competition.\\n\"\n",
            "\"Wow, congratulations Frank!\\n\"\n",
            "\"Thank you Carol.\\n\"\n",
            "\"Shall we have a pizza later to celebrate?\\n\"\n",
            "\"Sure. Do you recommend any restaurant, Carol?\\n\"\n",
            "\"Yes, a new restaurant opened, and they say the banana pizza is phenomenal.\\n\"\n",
            "\"Okay. Let's meet at the restaurant at seven PM, is that okay?\\n\"\n",
            "\"That's fine. See you later then.\"\n",
            "['[CLS]', '[MASK]', 'a', 'new', 'restaurant', 'opened', 'and', 'they', 'say', 'the', 'banana', 'pizza', 'is', 'phenomenal\\\\n\"', '[SEP]', '\"hello', 'carol', 'my', 'name', 'is', '[MASK]', 'nice', 'to', 'meet', '[MASK]', '[SEP]']\n",
            "List of Real Masked Tokens:  [52, 39, 18, 21]\n",
            "List of Predicted Masked Tokens:  []\n",
            "isNext (True Value):  False\n",
            "isNext (Predicted Value):  True\n"
          ]
        }
      ]
    }
  ]
}