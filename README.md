# psyych-llm
Using this repository to advance my knowledge on LLMs with examples, and more specifically dev a lmm with psyhcology degreee


# **LLM Pscyh' From Scratch**  


Welcome to the **psyych-llm ** repository! This project provides a basic implementation of a Large Language Model (LLM) architecture step by step while working with a psychological conversation dataset. By diving into this repository, you'll learn the fundamentals of Transformer-based architectures, the building blocks of LLMs, and how to train them effectively using Python and PyTorch. What's more, I aim to develop a CV project to detect neurodivergence based on handwritings. Something prior researchers has given up since it is mostly pseudoscience!!

---

## **Project Overview**  
Large Language Models (LLMs) have transformed the field of Natural Language Processing (NLP). This project provides a didactic approach to creating an LLM from scratch, covering:  
- Preprocessing text data.  
- Building embedding layers.  
- Implementing multi-head attention and encoder layers.  
- Training the architecture with a small dataset.  

---

## **Key Features**  
1. **Hands-on Learning**: Implement each module of the Transformer architecture from scratch.  
2. **Comprehensive Explanation**: All steps are well-documented with detailed comments and markdown cells in the Jupyter Notebook.  
3. **Dataset Included**: A toy dataset of psychological aid sentences within 'text.txt' is provided to simplify the training process and allow quick experimentation.  
4. **End-to-End Process**: From data preprocessing to training and evaluating the model, all steps are included.  

---

## **Getting Started**  

### **Prerequisites**  
Make sure you have the following installed:  
- Python 3.8 or later  
- PyTorch  
- NumPy  
- A Jupyter Notebook environment -> ( I personally used Collab )


## **Project Structure**  
The repository is structured as follows:  
```plaintext
LLM-From-Scratch/
├── LLM_From_Scratch.ipynb  # Main Jupyter Notebook for the project
├── text.txt                # Dataset used for training
└── README.md               # Repository documentation
```


## **Results and Insights**  
- **Model Training**: The provided implementation demonstrates how an LLM architecture learns patterns in a small dataset.  
- **Limitations**: Due to the limited data size and computational resources, the model is not optimized for high-performance tasks but provides a solid understanding of LLM design principles.  

---

## **Contributing**  
Contributions are welcome! Feel free to submit a pull request or open an issue to discuss improvements or ideas.


---

## **Acknowledgments**  
- **PyTorch Team**: For providing an exceptional framework for deep learning.  
- **Hugging Face**: For inspiring the NLP community with pre-trained models.  
- **BERT Paper**: *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding* (2018).
- **OpenAI**: I used ChatGPT to add line breaks after sentences <3

---
